I recommend using bash. The default shell in the servers is tsch for some reason, but bash can be run by just typing "bash". 
If you choose to work with tsch, just add the the relevant lines to ~/.tcshrc instead of ~/.bashrc and everything else should be the same (but I can't guarantee it).

************* Conda *************
It's easier to install conda in the correct path if we create a link: 
```
mkdir /home/joberant/NLP_2324b/<NAME>/miniconda3
ln -s /home/joberant/NLP_2324b/<NAME>/miniconda3 ~/miniconda3
```

Now we can follow the steps from here (https://docs.anaconda.com/miniconda/#quick-command-line-install):
```
curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
```

************* Cache setup *************
Environment variables:
Run in shell (can also add to ~/.bashrc):
```
export CONDA_CACHE_DIR=/home/joberant/NLP_2324b/<NAME>/.cache
export PIP_CACHE_DIR=/home/joberant/NLP_2324b/<NAME>/.cache
```

Create file `~/.condarc` with the following content:
```
pkgs_dirs:
  - /home/joberant/NLP_2324b/<NAME>/.cache/pkgs
envs_dirs:
  - /home/joberant/NLP_2324b/<NAME>/.cache/envs
```

Check where the caches are:
```
pip cache info
conda info
```

*** HuggingFace ***
Add to ~/.bashrc
```
export HF_HOME=/home/joberant/NLP_2324b/<NAME>/.cache/huggingface
```
Run:
```
mkdir /home/joberant/NLP_2324b/<NAME>/.cache/huggingface
ln -s /home/joberant/NLP_2324b/<NAME>/.cache/huggingface ~/.cache/huggingface
```



************* Creating the env *************
I created my env like this:
```
conda create -n <NAME> python=3.8
```

Then inside I used pip to install the packages I needed:
```
conda activate <NAME>
conda install -c conda-forge spacy
python -m pip install numpy torch sentence-transformers transformers openai rank-bm25 nltk ipython

python -m spacy download en_core_web_sm
```

Now, hopefully, running the interactive python interpreter on a server with a GPU should work.